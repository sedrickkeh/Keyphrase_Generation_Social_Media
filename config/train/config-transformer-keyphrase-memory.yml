model_type: keyphrase
memory: True
tgt_type: verbatim_append

data: data/keyphrase/meng17/kp20k
save_checkpoint_steps: 10000
keep_checkpoint: 40
seed: 3435

encoder_type: transformer
decoder_type: transformer
word_vec_size: 768
rnn_size: 768
layers: 4

position_encoding: true

optim: adam
learning_rate: 2
param_init: 0.1
warmup_steps: 8000
decay_method: noam
label_smoothing: 0.1
adam_beta2: 0.998
param_init_glorot: True

batch_type: tokens
normalization: tokens
max_generator_batches: 2
accum_count: 4

# batch_size is actually: num_example * max(#word in src/tgt)
batch_size: 4096
#batch_size: 8192
#batch_size: 24576
valid_batch_size: 256

train_steps: 300000
valid_steps: 10000
report_every: 100

#dropout: 0.2

share_embeddings: 'true'
copy_attn: 'true'
param_init_glorot: 'true'
log_file_level: DEBUG
tensorboard: 'true'

exp: kp20k-one2seq-cdkgen
save_model: models/kp20k/kp20k.one2seq.cdkgen
log_file: output/kp20k.one2seq.cdkgen.log
tensorboard_log_dir: runs/kp20k.one2seq.cdkgen/

world_size: 4
gpu_ranks:
- 0
- 1
- 2
- 3
#master_port: 10000
