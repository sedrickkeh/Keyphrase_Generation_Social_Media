model_type: keyphrase
memory: True
tgt_type: verbatim_append

data: data/keyphrase/meng17/twitter
save_checkpoint_steps: 5000
keep_checkpoint: 40
seed: 3435

encoder_type: transformer
decoder_type: transformer
word_vec_size: 768
rnn_size: 768
layers: 4

position_encoding: true

optim: adam
learning_rate: 2
param_init: 0.1
warmup_steps: 8000
decay_method: noam
label_smoothing: 0.1
adam_beta2: 0.998
param_init_glorot: True

batch_type: tokens
normalization: tokens
max_generator_batches: 2
accum_count: 4

# batch_size is actually: num_example * max(#word in src/tgt)
batch_size: 4096
#batch_size: 8192
#batch_size: 24576
valid_batch_size: 256

train_steps: 40000
valid_steps: 10000
report_every: 200

#dropout: 0.2

share_embeddings: 'true'
copy_attn: 'true'
param_init_glorot: 'true'
log_file_level: DEBUG
tensorboard: 'true'

exp: twitter_0
save_model: models/twitter_0/twitter
log_file: models/twitter_0/twitter.log
tensorboard_log_dir: runs/twitter_0/

world_size: 1
gpu_ranks:
- 0
#- 1
#- 2
#- 3
#master_port: 10000
